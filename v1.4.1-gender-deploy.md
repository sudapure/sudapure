This release contains baseline TensorRT implementation of pre-trained openvino model provided by Intel as per [here](https://docs.openvino.ai/latest/omz_models_model_person_attributes_recognition_crossroad_0230.html), which presents a person attributes classification algorithm analysis scenario. It produces probability of person attributions existing on the sample and a position of two point on sample, which in this case used for gender classification. This release doesn't based on any of research releases as this release entirely focuses on deploying pre-trained model in openvino framework to native TensorRT version, this release was supposed to be published on 08/12/2022 but due to PR reviews & other priority tasks it got delayed.

## Change Logs

- Deploy OpenVino M.A.R model to TensorRT Zoo Models by @sudapure in https://github.com/onearmbandit/VAS-Video-Analytics/pull/112
- CPP version of ONNX to tensorRT converter by @sudapure in https://github.com/onearmbandit/VAS-Video-Analytics/pull/114

## Issues Covered

-  Tracking the deployment ready NN models using DVC irrespective of source/original development or Inference Engine frameworks. :heavy_check_mark::1st_place_medal: 
-  Implementing & maintaining helper scripts/modules for pre/post processing required for model inference. :heavy_check_mark::1st_place_medal: 
-  Maintaining stand alone inference script's/modules which will simply infer the model with sample demo application irrespective of the source framework or Inference Engines(But for now it will be only for TensorRT). :heavy_check_mark::1st_place_medal: 
- Independent script/module for native inference engine serialization for target precision's FP32/FP16/INT8 :heavy_check_mark::1st_place_medal: 
- Maintaining scripts/modules for accuracy/latency validation benchmark for cross framework comparison. :heavy_check_mark::1st_place_medal: 
- Maintaining and keeping original `LICENSE` associated with the pre-trained models before utilizing it for the commercial purposes. :heavy_check_mark::1st_place_medal: 
- Validating & bench-marking the OpenVino model with the existing Core Lib models w.r.t speed/accuracy tradeoff. :heavy_check_mark::1st_place_medal: 
- Validating & bench-marking the OpenVino model with the TensorRT converted model w.r.t speed/accuracy tradeoff across target precision's FP32/FP16/INT8. :heavy_check_mark::1st_place_medal: 
- Building quantize engine weights using INT8 network definition as per PR https://github.com/vicon-security/VAS-Video-Analytics/pull/75. :heavy_check_mark::1st_place_medal: 
- TensorRT engine serialization support for FP32/FP16/INT8(PTQ) :heavy_check_mark::1st_place_medal: 

## Specification
<table>
<tr>
<th>Metric</th>
<th>Value</th>
</tr>
<tr>
<td>f1-score accuracy</td>
<td>91 (<a href="https://docs.openvino.ai/latest/omz_models_model_person_attributes_recognition_crossroad_0230.html#accuracy">Openvino Doc</a>)</td>
</tr>
<tr>
<td>Pose coverage</td>
<td>Standing upright, parallel to image plane</td>
</tr>
<tr>
<td>Camera Angle</td>
<td><= &ang;40&#176;</td>
</tr>
<tr>
<td>Min object width</td>
<td>80 pixels</td>
</tr>
<tr>
<td>Support of occluded pedestrians </td>
<td>Yes</td>
</tr>
<tr>
<td>Occlusion coverage  </td>
<td> <20% </td>
</tr>
<tr>
<td>GFlops  </td>
<td>0.174 </td>
</tr>
<tr>
<td>MParams   </td>
<td>0.735 </td>
</tr>
<tr>
<td>Source framework </td>
<td>PyTorch*</td>
</tr>
</table>

## Implementation
This release is primarily intended for storing/tracking models using DVC that has been inter-converted between various frameworks along with inference module for quick evaluation and testing. Supported frameworks are TensorFlow, PyTorch, ONNX, OpenVINO, TFJS, TFTRT, TensorFlowLite (Float32/16/INT8), etc. This repo will track pre-trained NN models from different modalities trained or deployed in various frameworks and IE's(Inference Engines). As per existing observations that been put forward, the Nvidia NGC catalog model registry hosts several pre-trained serialized models that could directly served for specific business use-case. However not just Nvidia but model catalog registry for pre-trained NN's from OpenVino, ONNX(-runtime) TensorFlow(TFlite), MXNet provides wide range of deployment ready models which can address to multiple domain specific business requirements, following things are covered in this release,

-   Tracking the deployment ready NN models using DVC irrespective of source/original development or Inference Engine frameworks.
-   Implementing & maintaining helper scripts/modules for pre/post processing required for model inference.
-    Maintaining stand alone inference script's/modules which will simply infer the model with sample demo application irrespective of the source framework or Inference Engines(But for now it will be only for TensorRT).
-   The code base will be based on simple design pattern, easy to use for naive developers.
-   Independent script/module for native inference engine serialization for target precision's FP32/FP16/INT8
-   Custom parser modules for model interoperability between different inference frameworks(mostly ONNX parsers will be used).
-   Maintaining scripts/modules for accuracy/latency validation benchmark for cross framework comparison.
-   Every use-case will have demo sample for show-case utility.
-   Maintaining and keeping original `LICENSE` associated with the pre-trained models before utilizing it for the commercial purposes.

### DataSet Used
As this release is targeted for gender classification use-case, so as per openvino pre-trained model documentation proprietary dataset is used for training and testing the model however for cross validating the model accuracy across baseline implementation as per release [v1.4-gender-deploy](https://github.com/vicon-security/VAS-Video-Analytics/releases/tag/v1.4-gender-deploy) PR https://github.com/onearmbandit/VAS-Data-Science/pull/327 + https://github.com/onearmbandit/VAS-Data-Science/pull/373 and https://github.com/onearmbandit/VAS-Data-Science/pull/278 along with benchmark M.A.R [v0.1-alpha](https://github.com/onearmbandit/VAS-Data-Science/releases/tag/v0.1-alpha) dataset used for accuracy validation.

### Architecture Design
Below is ONNX representation of the model NN architecture design after exporting the pre-trained openvino weights to intermediate ONNX representation and visualized using netron app. Below architecture diagram summarizes entire NN backbone network along with classification head for all the supported attributes at output nodes.
![person-attributes-recognition-crossroad-0230_fp32 onnx](https://user-images.githubusercontent.com/57352045/207428594-250978ab-48ee-4092-a271-f50de6c60ac9.png)

### Inputs

Image, name: `0`, shape: `1, 3, 160, 80` in the format `1, C, H, W`, where:
Openvino model expects channel firs order but TensorRT inference requires channel last order, however `tools/onnx_to_tensorrt.py` pre-processing takes channel first order only.

- `C` - number of channels
- `H` - image height
- `W` - image width

The expected color order is `BGR`.

### Outputs

1.  The net outputs a blob named `453` with shape: `1, 8, 1, 1` across eight attributes:
    [`is_male`, `has_bag`, `has_backpack`, `has_hat`, `has_longsleeves`, `has_longpants`, `has_longhair`,
     `has_coat_jacket`]. Value > 0.5 means that an attribute is present.
2.  The net outputs a blob named `456` with shape: `1, 2, 1, 1`. It is location of point with top color.
3.  The net outputs a blob named `459` with shape: `1, 2, 1, 1`. It is location of point with bottom color.


## Source IE ->  ONNX -> TensorRT/TFLite 
As OpenVINO is very popular IE framework available, it provides huge set of pre-trained models as compared to any other framework or IE but converting the model from native Openvino format to any other target platform or hardware architecture's is not a straight forward process, however model conversion might not be a subject to implementation for this release. but tracking the already converted model mostly from native OpenVIno/Tensorflow IR to TensorRT/TFLite,etc. will be considered here.

### Source IE ->  ONNX
As this release primarily emphasizes on tracking different models which are already converted from the source framework, hence converting the model from native Openvino format to intermediate ONNX representation requires to build custom NN specific parsers which is currently not in the scope of this release, Might cover in the near future release.

### ONNX -> TensorRT
As most of the models currently deployed using TensorRT IE as a subject to VAS project F.R.D's, hence converting the model from intermediate ONNX representation to native TensorRT engine is covered inside the `tools` package with appropriate python script. currently Python implementation is provided not only for research purpose, but also supported the CPP implementation in order to ship the binaries for engine serialization component required to the product team.

## Application Utility
### Gender Classification
For quick evaluation of trained  model the classifier output is used with some selected input images to classify the images as Male or Female, below are two scenarios where in first one the query image & retrieved results are close enough as in the second case the input & classified images are pretty distinct indicating as worst case scenario.


<table class="waffle" cellspacing="0" cellpadding="0">
    <tbody>
        <tr align="center">
            <td class="s0" dir="ltr" colspan="4"> Best Case </td>
            <td class="s0" dir="ltr" colspan="4"> Worst Case </td>
        </tr>
        <tr align="center">
            <td class="s0" dir="ltr">Category</td>
            <td class="s0" dir="ltr" colspan="3">Results</td>
            <td class="s0" dir="ltr">Category</td>
            <td class="s0" dir="ltr" colspan="3">Results</td>
        </tr>
        <tr align="center">
            <td class="s1" dir="ltr">Male</td>
            <td class="s2" dir="ltr"><img src="https://user-images.githubusercontent.com/72791642/190174154-ba3aea20-f3b7-4659-a2a5-ebe9defdadda.png" width="125" height="300"></td>
            <td class="s3" dir="ltr"><img src="https://user-images.githubusercontent.com/72791642/190174565-d56a5412-97e3-4cf9-8b70-8b71b8b2f101.png" width="125" height="300"></td>
            <td class="s3" dir="ltr"><img src="https://user-images.githubusercontent.com/72791642/190174723-b866cca3-7a29-4805-ba1d-39cea62ba0e0.png" width="125" height="300"></td>
            <td class="s1" dir="ltr">Male</td>
            <td class="s2" dir="ltr"><img src="https://user-images.githubusercontent.com/72791642/190178015-11e92fbb-325e-4a7f-b401-38a54120e7c8.png" width="125" height="300"></td>
            <td class="s3" dir="ltr"><img src="https://user-images.githubusercontent.com/72791642/190178366-c1421e86-11bf-4e20-bca1-3f0c0375bb46.png" width="125" height="300"></td>
            <td class="s3" dir="ltr"><img src="https://user-images.githubusercontent.com/72791642/190178306-765e41d5-e048-4ad9-bbaa-6b2eb7775da0.png" width="125" height="300"></td>
        </tr>
        <tr align="center">
            <td class="s1" dir="ltr">Female</td>
            <td class="s3" dir="ltr"><img src="https://user-images.githubusercontent.com/72791642/190177182-6158ebd0-5a4a-49c4-a609-cf6ed5d06b4a.png" width="125" height="300"></td>
            <td class="s3" dir="ltr"><img src="https://user-images.githubusercontent.com/72791642/190177364-d0defa49-41ce-4fe0-8f46-cd61e5ac1098.png" width="125" height="300"></td>
            <td class="s3" dir="ltr"><img src="https://user-images.githubusercontent.com/72791642/190177562-974b2218-651e-4bb6-8462-b53fadb17960.png" width="125" height="300"></td>
            <td class="s1" dir="ltr">Female</td>
            <td class="s3" dir="ltr"><img src="https://user-images.githubusercontent.com/72791642/190178699-9deb75a4-f350-4f2f-806e-404fff5cedd3.png" width="125" height="300"></td>
            <td class="s3" dir="ltr"><img src="https://user-images.githubusercontent.com/72791642/190178751-f600c05e-c968-4b81-87b2-2299d6c17f68.png" width="125" height="300"></td>
            <td class="s3" dir="ltr"><img src="https://user-images.githubusercontent.com/72791642/190178765-d1b03848-2639-488d-b829-6399d959e819.png" width="125" height="300"></td>
        </tr>
    </tbody>
</table>


## Evaluation Metrics
As primary objective of this release implementation is to achieve similar output tensor precision as that of OpenVino based pre-trained model, the OpenVino model for M.A.R [person-attributes-recognition-crossroad-0230](https://docs.openvino.ai/latest/omz_models_model_person_attributes_recognition_crossroad_0230.html)(gender classification) is been cross-evaluated against the converted/deployed TensorRT model, As per the [PR#75](https://github.com/onearmbandit/VAS-Video-Analytics/pull/75#issue-1216863984), the required calibration dataset for INT8 inference has been pulled from PR [#256](https://github.com/onearmbandit/VAS-Data-Science/pull/256) & [#262](https://github.com/onearmbandit/VAS-Data-Science/pull/262) from [onearmbandit/VAS-Data-Science](https://github.com/onearmbandit/VAS-Data-Science) repo, ideally the calibration dataset is assumed to be subset of test/val dataset and the results(accuracy) are almost identical as mentioned in below table.

<table cellspacing="0" cellpadding="0">
        <tbody>
            <tr align="center"> 
                <td class="s0" dir="ltr" rowspan="3">Use-Case</td>
                <td class="s0" dir="ltr" rowspan="3">DataSet</td>
                <td class="s0" dir="ltr" colspan="4">OpenVIno (Intel(R) Core(TM) i7-8700 CPU @ 3.20GHz)</td>
                <td class="s0" dir="ltr" colspan="4">TensorRT (NVIDIA GeForce GTX 1060 6GB)</td>
            </tr>
            <tr align="center">
                <td class="s0" dir="ltr" colspan="2">FP32</td>
                <td class="s0" dir="ltr" colspan="2">INT8</td>
                <td class="s0" dir="ltr" colspan="2">FP32</td>
                <td class="s0" dir="ltr" colspan="2">INT8</td>
            </tr>
            <tr align="center">
                <td class="s0" dir="ltr">Accuracy</td>
                <td class="s0" dir="ltr">Latency (ms)</td>
                <td class="s0" dir="ltr">Accuracy</td>
                <td class="s0" dir="ltr">Latency (ms)</td>
                <td class="s0" dir="ltr">Accuracy</td>
                <td class="s0" dir="ltr">Latency (ms)</td>
                <td class="s0" dir="ltr">Accuracy</td>
                <td class="s0" dir="ltr">Latency (ms)</td>
            </tr>
            <tr align="center">
                <td class="s1" dir="ltr">Gender-Net</td>
                <td class="s1" dir="ltr"><a href="https://github.com/onearmbandit/VAS-Data-Science/pull/327">327</a> + <a href="https://github.com/onearmbandit/VAS-Data-Science/pull/373">373</a></td>
                <td class="s0" dir="ltr">78(M)/82(F)/80</td>
                <td class="s0" dir="ltr">1.04 ms</td>
                <td class="s0" dir="ltr">78(M)/82(F)/80</td>
                <td class="s0" dir="ltr">1.09 ms</td>
                <td class="s0" dir="ltr">79(M)/82(F)/80</td>
                <td class="s0" dir="ltr">0.90 ms</td>
                <td class="s0" dir="ltr">78(M)/82(F)/80</td>
                <td class="s0" dir="ltr">0.80 ms</td>
            </tr>
            <tr align="center">
                <td class="s1" dir="ltr">Gender-Net</td>
                <td class="s1" dir="ltr"> <a href="https://github.com/onearmbandit/VAS-Data-Science/pull/278">278</a></td>
                <td class="s0" dir="ltr">75(M)/70(F)/73</td>
                <td class="s0" dir="ltr">1.26 ms</td>
                <td class="s0" dir="ltr">74(M)/70(F)/72</td>
                <td class="s0" dir="ltr">1.13 ms</td>
                <td class="s0" dir="ltr">75(M)/70(F)/73</td>
                <td class="s0" dir="ltr">1.03 ms</td>
                <td class="s0" dir="ltr">74(M)/70(F)/72</td>
                <td class="s0" dir="ltr">0.93 ms</td>
            </tr>
        </tbody>
</table>

**NOTE**: Both the models in above table has inferred with batch size of 1, and both openvino and TensorRT model infers at input resolution of `160 x 80`(HxW) for every input sample. For TensorRT PTQ(INT8) the required calibration dataset for INT8 inference has been pulled from PR https://github.com/onearmbandit/VAS-Data-Science/pull/256 & https://github.com/onearmbandit/VAS-Data-Science/pull/262 from [onearmbandit/VAS-Data-Science](https://github.com/onearmbandit/VAS-Data-Science) repo.

The TensorRT deployed model for M.A.R [person-attributes-recognition-crossroad-0230](https://docs.openvino.ai/latest/omz_models_model_person_attributes_recognition_crossroad_0230.html)(gender classification) which is converted from native OpenVino model, is been cross-evaluated against the model which is deployed as per deployment release [v1.4-gender-deploy](https://github.com/vicon-security/VAS-Video-Analytics/releases/tag/v1.4-gender-deploy) which is based on research release [v1.6-beta](https://github.com/vicon-security/VAS-Core-Research/releases/tag/v1.6-beta), and the comparative results for speed/accuracy tradeoff as mentioned in below table.

<table cellspacing="0" cellpadding="0">
        <tbody>
            <tr align="center"> 
                <td class="s0" dir="ltr" rowspan="3">Use-Case</td>
                <td class="s0" dir="ltr" rowspan="3">DataSet</td>
                <td class="s0" dir="ltr" colspan="4">TensorRT (<a href="https://github.com/vicon-security/VAS-Video-Analytics/releases/tag/v1.4-gender-deploy">v1.4-gender-deploy</a>) (NVIDIA GTX 1060)</td>
                <td class="s0" dir="ltr" colspan="4">TensorRT (<a href="https://docs.openvino.ai/latest/omz_models_model_person_attributes_recognition_crossroad_0230.html">OpenVino Model</a>) (NVIDIA GTX 1060)</td>
            </tr>
            <tr align="center">
                <td class="s0" dir="ltr" colspan="2">FP32</td>
                <td class="s0" dir="ltr" colspan="2">INT8</td>
                <td class="s0" dir="ltr" colspan="2">FP32</td>
                <td class="s0" dir="ltr" colspan="2">INT8</td>
            </tr>
            <tr align="center">
                <td class="s0" dir="ltr">Accuracy</td>
                <td class="s0" dir="ltr">Latency (ms)</td>
                <td class="s0" dir="ltr">Accuracy</td>
                <td class="s0" dir="ltr">Latency (ms)</td>
                <td class="s0" dir="ltr">Accuracy</td>
                <td class="s0" dir="ltr">Latency (ms)</td>
                <td class="s0" dir="ltr">Accuracy</td>
                <td class="s0" dir="ltr">Latency (ms)</td>
            </tr>
            <tr align="center">
                <td class="s1" dir="ltr">Gender-Net</td>
                <td class="s1" dir="ltr"><a href="https://github.com/onearmbandit/VAS-Data-Science/pull/327">327</a> + <a href="https://github.com/onearmbandit/VAS-Data-Science/pull/373">373</a></td>
                <td class="s0" dir="ltr">73(M)/82(F)/78</td>
                <td class="s0" dir="ltr">1.59 ms</td>
                <td class="s0" dir="ltr">74(M)/81(F)/78</td>
                <td class="s0" dir="ltr">1.17 ms</td>
                <td class="s0" dir="ltr">79(M)/82(F)/80</td>
                <td class="s0" dir="ltr">0.90 ms</td>
                <td class="s0" dir="ltr">78(M)/82(F)/80</td>
                <td class="s0" dir="ltr">0.80 ms</td>
            </tr>
            <tr align="center">
                <td class="s1" dir="ltr">Gender-Net</td>
                <td class="s1" dir="ltr"> <a href="https://github.com/onearmbandit/VAS-Data-Science/pull/278">278</a></td>
                <td class="s0" dir="ltr">91(M)/87(F)/90</td>
                <td class="s0" dir="ltr">1.59 ms</td>
                <td class="s0" dir="ltr">91(M)/87(F)/90</td>
                <td class="s0" dir="ltr">1.19 ms</td>
                <td class="s0" dir="ltr">75(M)/70(F)/73</td>
                <td class="s0" dir="ltr">1.03 ms</td>
                <td class="s0" dir="ltr">74(M)/70(F)/72</td>
                <td class="s0" dir="ltr">0.93 ms</td>
            </tr>
        </tbody>
</table>

As from above tables, there is slight improvement in latency number for inferring in INT8 mode on GTX 1060 card these number could improve even better on RTX-30xx series cards which could be evaluated seperately,

**NOTE**: Both the models in above table has inferred with batch size of 1, and OpenVino-TRT infers at resolution of `160 x 80`(HxW) and <a href="https://github.com/vicon-security/VAS-Video-Analytics/releases/tag/v1.4-gender-deploy">v1.4-gender-deploy</a> model infers at input resolution of `224 x 224`(HxW) for every input sample. For TensorRT PTQ(INT8) the required calibration dataset for INT8 inference has been pulled from PR https://github.com/vicon-security/VAS-Data-Science/pull/256 & https://github.com/vicon-security/VAS-Data-Science/pull/262 from [vicon-security/VAS-Data-Science](https://github.com/vicon-security/VAS-Data-Science) repo.

## Conclusive Remark
The primary purpose of this release is to export the native Openvino model to intermediate ONNX represenation and serialize it to native TensorRT engine format along with inference module for testing and validating the accuracy benchmarks with respect to the deployment release [v1.4-gender-deploy](https://github.com/vicon-security/VAS-Video-Analytics/releases/tag/v1.4-gender-deploy). The expected outcome of this release is getting the near similar benchmark number for accuracy by optimizing the infer latency using TensorRT specific implementation for as that of the pre-trained openvino model. Both the infer latency & accuracy has been emphasized for this release as a part of deployment phase,  the accuracy numbers achieved for this release are much more balance with respect to infer latency as compared to the previous gender classification release.

## Known Issues
- As per observations model performs well on training and validation dataset but when tested on real time dataset accuracy of the model drops significantly (but this is much less as compared to previous release).
- Lack of discriminative inter-class features makes gender classification difficult.
- Full body appearance makes hard for classification model to correctly classify between male & female class.
- Model produces false results for input image below certain A.R.
- As the end-to-end network is cascaded along with detection model, the cascading effect adds significant toll in latency, which increases end-to-end FPS & latency numbers.
- Model is not tested with Reviewed Dataset, however diversified source dataset is used for accuracy validation.

## Further improvements (#TODO):
- Improving accuracy without TTA, which is crucial for deployment specific releases.
- NN architecture optimization & decreasing trainable parameter to further improve latency numbers
- Implementation of custom NN operators to optimize infer latency by keeping accuracy intact.
- Implementation of Model pruning strategy for optimizing infer latency by manupulating the sprasity of the NN weights.
