# Concurrent Gender Classification & Person ReID Release based on IBN-Net Architecture
This release is based on follow-up development of Gender classification and Person ReID based retrieval problem optimization, the primary goal of this release is to unify the architectural implementation of both Gender classification and Person Retrieval problem by sharing common backbone architecture to optimize the latency for both the use-cases, the space complexity has also targeted to optimize the memory footprints of feature embedding that are feed to the use-case specific NN head. This release was supposed to be published on 05/9/2022 but due to PR reviews & other priority tasks it got delayed, the experimentation's covered in this release signifies to preliminary implementation efforts in optimizing person Re-ID & Gender classification problem.

## Change Logs
* Concurrent training for gender and Person re-identification by @sarveshamberkarC in https://github.com/onearmbandit/VAS-Core-Research/pull/73
* baseline implementation of custom hardware aware NN in pytorch by @sudapure in https://github.com/onearmbandit/VAS-Core-Research/pull/76
* MobileOne - An Improved One millisecond Mobile Backbone Pytorch implementation by @sudapure in https://github.com/onearmbandit/VAS-Core-Research/pull/78

## Issues Resolved
- Unifying the backbone architecture to handle gender & ReID use-case problem using single go.:heavy_check_mark: :1st_place_medal:
- Implementation of gender classifier head along with Re-ID head sharing common backbone architecture :heavy_check_mark: :1st_place_medal:
- Space complexity optimization by reducing the feature embedding from 512-d to 128-d dimensions.:heavy_check_mark: :1st_place_medal:
- Modifying dataloader module to address use-case unification implementation :heavy_check_mark: :1st_place_medal:
- Design and Implementation of Concurrent multi-task training strategy  for IBN-net-a, GenNet, RMnet. :heavy_check_mark: :1st_place_medal:
- Implementation of auxiliary branching for IBNnet-a [here](https://github.com/onearmbandit/VAS-Core-Research/pull/73#issuecomment-1180613351). :heavy_check_mark: :1st_place_medal:
- Design and Implementation multi-task Data-Loader. :heavy_check_mark: :1st_place_medal:
- Implementation of mask transformation [here](https://github.com/onearmbandit/VAS-Core-Research/pull/73/commits/939a56debc2ca15eefb7cf61902abb60de0e4f6f). :heavy_check_mark: :1st_place_medal:
- Implementation of Metric loss function namely Circle Loss, Triplet Loss, Center loss :heavy_check_mark: :1st_place_medal:
- Experimentation and evaluation on various architecture for Person -reid and gender task [here](https://docs.google.com/document/d/1OMxwlCedrsxOHhfWJDdXBUI44zozGDlpwGGIOMNAa_A/edit#) :heavy_check_mark: :1st_place_medal:
- EDA report for the data used for [reid](https://docs.google.com/document/d/1fcsSjSZrgnCO_dmdfUvFjYmnYJ0u9XRcCCfsROty0Jk/edit?usp=sharing) and [gender classification task](https://docs.google.com/document/d/1fV_PbbugZrKx49APCgoKZ8OsmR5Hsbb6rsbso6PLx48/edit?usp=sharing) :heavy_check_mark: :1st_place_medal:
- Implementation of multi-level auxiliary branching for IBNnet-a [here](https://github.com/onearmbandit/VAS-Core-Research/pull/73#issuecomment-1199202866) :heavy_check_mark: :1st_place_medal:
- Implementation of Re-ranking k-reciprocal neighbors: unsupervised post-processing [here](https://github.com/onearmbandit/VAS-Core-Research/pull/73#issuecomment-1181942768). :heavy_check_mark: :1st_place_medal:
- Implementation of different distance metrics namely Cosine, Triplet and euclidean distance. :heavy_check_mark: :1st_place_medal:
- Resolved the issue of fluctuation in accuracy for person ReID due to different batch size [here](https://github.com/onearmbandit/VAS-Core-Research/pull/73#issuecomment-1199246176) :heavy_check_mark: :1st_place_medal:
- added gender prediction in image_retrieval script. :heavy_check_mark: :1st_place_medal:

## Specification
<table>
<tr>
<th>Metric</th>
<th>Value</th>
</tr>
<tr>
<td>Pose coverage</td>
<td>Standing upright, parallel to image plane</td>
</tr>
<tr>
<td>Camera Angle</td>
<td><= &ang;40&#176;</td>
</tr>
<tr>
<td>Support of occluded pedestrians </td>
<td>Yes</td>
</tr>
<tr>
<td>Occlusion coverage  </td>
<td> <50% </td>
</tr>
<tr>
<td>GFlops  </td>
<td>N/A </td>
</tr>
<tr>
<td>MParams   </td>
<td>N/A </td>
</tr>
<tr>
<td>Source framework </td>
<td>PyTorch </td>
</tr>
</table>

## Dataset used
### Gender Classification
Dataset from PR https://github.com/onearmbandit/VAS-Data-Science/pull/278 & PR https://github.com/onearmbandit/VAS-Data-Science/pull/279 was used for training the model, this dataset is generated from object365 detection dataset for gender and multi-attribute classification use-case, and M.A.R [v0.1-alpha](https://github.com/onearmbandit/VAS-Data-Science/releases/tag/v0.1-alpha) used for test evaluation of the model. dataset in release [v0.1-alpha](https://github.com/onearmbandit/VAS-Data-Science/releases/tag/v0.1-alpha) is a large-scale public benchmark dataset for M.A.R. The dataset contains 33507 images, and this dataset version is created by filtering and rectifying all the images from PETA RAP public dataset.
<table><td>
<img src="https://user-images.githubusercontent.com/57352045/120796875-f8638e00-c558-11eb-922f-4450a2651737.png"/></td>
<td>
<img src="https://user-images.githubusercontent.com/57352045/120797283-732ca900-c559-11eb-8469-bd9e77dbfd1e.png"/></td>
<td>
<img src="https://user-images.githubusercontent.com/57352045/120797438-aa9b5580-c559-11eb-8624-71e5871b2fb9.png"/></td>
</table>

**NOTE**: _**The number of data samples showed in above graphs are only from the training datasets(excluding https://github.com/onearmbandit/VAS-Data-Science/pull/278 & PR https://github.com/onearmbandit/VAS-Data-Science/pull/279). for detailed graphical analysis of complete dataset(train/test/val) has described in the [E.D.A document](https://github.com/onearmbandit/VAS-Data-Science/releases/download/v0.1-alpha/PETA.and.RAP.Dataset.EDA.Report.pdf) respectively**. Dataset used is not manually verified., other required dataset is currently in progress as under PR & issue on [onearmbandit/VAS-Data-Science](https://github.com/onearmbandit/VAS-Data-Science)_.
### Person Re-Identification
Dataset from PR [#264](https://github.com/onearmbandit/VAS-Data-Science/pull/264) was used for training. This dataset is a combination of multiple PRs : [#229](https://github.com/onearmbandit/VAS-Data-Science/pull/229) [#230](https://github.com/onearmbandit/VAS-Data-Science/pull/230) [#231](https://github.com/onearmbandit/VAS-Data-Science/pull/231) [233](https://github.com/onearmbandit/VAS-Data-Science/pull/233) [#245](https://github.com/onearmbandit/VAS-Data-Science/pull/245) [#247](https://github.com/onearmbandit/VAS-Data-Science/pull/247) [#251](https://github.com/onearmbandit/VAS-Data-Science/pull/251).The data is collected from Open-source datasets, namely CHUK03, Market1501, DUKE. CHUK03 dataset consist of 1360 identities and 13164 images , Market1501 dataset consists of 1500 IDs and 32,668 image samples, DUKE dataset consists of 16,522 training images of 702 identities, 2,228 query images of the other 702 identities and 17,661 gallery images.
**Note:** _The Above image numbers depict the dataset before manual verification_ 
The dataset used for additional benchmarking Includes EarthCam USA and MARS dataset. EarthCam USA dataset was developed in-house using the Live feed obtained from [Earthcam.com](earthcam.com) The raw data was collected using the tracking script available on PR [#271]( https://github.com/onearmbandit/VAS-Data-Science/pull/271) This data was then manually verified on PR [#283](https://github.com/onearmbandit/VAS-Data-Science/pull/283),  [#325](https://github.com/onearmbandit/VAS-Data-Science/pull/325) as this data was collected from a single camera view therefore number of camera IDs in the dataset is 1.
MARS dataset  has been collected from six near-synchronized cameras. It consists of 1,261 different pedestrians, who are captured by at least 2 cameras with variations in poses, colors and illuminations of pedestrians. 
The [Data Allocation](https://docs.google.com/spreadsheets/d/1-jXlLTMKlkL2TnXLINhfZSeY_1VKIvG1IjxEOpFl-AQ/edit#gid=0) sheet describes for which tasks the above datasets were used.

## Experimentation Details
Multiple Experimentation's were conducted with different NN architecture(IBN-net-A, RM-net, IBN-net-B) using different permutation and combination of the loss functions for both gender as well as person re-identification use-case. In all the experimentation conducted, IBN-net-A seems to perform better than other experiments with respect to inference cost and accuracy.Therefore this release focuses on IBN-Net-A architecture. For further details on conducted experimentation, refer [this](https://docs.google.com/document/d/1OMxwlCedrsxOHhfWJDdXBUI44zozGDlpwGGIOMNAa_A/edit#heading=h.m028if17sp5h) document 
IBN-Net is a novel convolutional architecture built upon the foundation of the ResNet-18 architecture. IBN-net uses Instance Batch normalization(IN) with a combination of vanilla batch normalization to produce high generalizable features for a given input query. Below is the architecture for ResNet-18, IBN-net-A and IBN-net-B.
<p align='center'>
<img src="https://user-images.githubusercontent.com/72791642/190011710-8e10557b-98c6-4f36-b36b-f15260127609.png"/>
</p>

### Main Objective
* The objective of the experiment was to unify the gender and person re-identification weights. 
* To reduce the dimension of the feature vector from 512 to 128 dimensions.

### Previous work 
The traditional method for multi-task weight sharing includes.
#### Common dataset for multi-task learning 
 In this case as per our use case we would have to manually annotate the person reid dataset for gender attributes.Annotating gender dataset for Person Re Identification is not feasible because , according to basic criteria for person re identification, at least 4 instances of same person should be present in the dataset. This creates dependency for manual labels also as Person re identification contains images of the same person instances this might also lead to dataset specific over-fitting.

#### Transfer Learning
 This method is a widely adopted method for weight sharing. In this method the upper layer of the model is frozen and the rest of the backbone is trained using existing dataset. Experiments were conducted for transfer learning, Person ReID weights were used as backbone for gender attribute tasks and vice versa, there was no significant gain in performance.The observed results of the same is available on [PR59](https://github.com/onearmbandit/VAS-Core-Research/pull/59).

### Data Pipeline 
We use two independent dataset for both the task. For person re-identification PR [#264](https://github.com/onearmbandit/VAS-Data-Science/pull/264) for gender task PR [#278](https://github.com/onearmbandit/VAS-Data-Science/pull/278) [#279](https://github.com/onearmbandit/VAS-Data-Science/pull/279) and M.A.R [v0.1-alpha](https://github.com/onearmbandit/VAS-Data-Science/releases/tag/v0.1-alpha) was used for training and testing. Both the dataset mentioned above are independent and do not share common images or annotation.The data-loader was designed to pass the task specific batches in random pattern such that both task are learned concurrently.

### Training 
The model has two auxiliary outputs namely for Person-Re identification and Gender classification. There are two independent optimizers for different tasks and gradients are calculated when a forward pass is performed on each task batch.As are these both relative in nature, the model is able to learn common feature for both the tasks,
Gender task uses the sum of Triplet and BCE loss function and Re-Identification task uses sum of Triplet + triplet query + Center and cross entropy loss. As there are separate optimizers for both the task the loss feedback to the model is also independent.Therefore, weights of the model are adjusted without hurting the accuracy of the other task. The model is evaluated at a set interval and the best model is saved for both the tasks.

### Loss function 
#### Gender 
##### Binary Cross Entropy With Logits Loss 
This loss combines a Sigmoid layer and the BCELoss in one single class.Binary cross entropy compares each of the predicted probabilities to actual class output which can be either 0 or 1 formula of the same is given below
<img src = https://user-images.githubusercontent.com/72791642/190061923-6cce90a2-afad-4ac4-bdb6-30f895db1bf7.png width="350" height="35">

##### Triplet Loss
Triplet loss used in this implementation is modified triplet loss mentioned in In paper Defense of the Triplet Loss for Person Re-Identification. Original implementation of the loss function can be found [here](https://github.com/Cysu/open-reid/blob/master/reid/loss/triplet.py). 
<img src = https://user-images.githubusercontent.com/72791642/190064299-e96f05a2-335a-4084-b68c-806ad39fa123.png width="350" height="50">


#### Person Re-Identification

##### Cross Entropy Loss
Cross-entropy loss 
When the number of classes ranges from [0, C] The unreduced loss for this case can be described as
<img src = https://user-images.githubusercontent.com/72791642/190063191-b323004b-90ea-4f84-a4d4-2691c409cfb3.png width="350" height="40">
where x is the input, y is the target, w is the weight, C is the number of classes, and N is the size of mini-batch.

##### Triplet Loss
The triplet loss used in this implementation is modified triplet loss mentioned in the paper Defense of the Triplet Loss for Person Re-Identification. The original implementation of the loss function can be found [here](https://github.com/Cysu/open-reid/blob/master/reid/loss/triplet.py). 
Along with triplet loss hard batch mining strategy was used to increase the distance between positive and negative anchors.
<img src = https://user-images.githubusercontent.com/72791642/190064299-e96f05a2-335a-4084-b68c-806ad39fa123.png width="350" height="50">

##### Triplet Loss query
This loss is calculated by position wise elimination of person instances and calculating triplet loss over the number of instances present in the batch

##### Center loss 
the center loss simultaneously learns a center for deep features of each class and penalizes the distances between the deep features and their corresponding class centers This idea was originally proposed in A _Discriminative Feature Learning Approach for Deep Face Recognition_
<h2>Implementation </h2>
The model was trained in two step process. First model was trained concurrently on gender and person re-identification task for 20 epochs and best model for person re-identification was achieved at 10th epoch. the model was then again fine-tuned on Gender task by freezing the complete backbone and training gender exclusive convolution layer for 5 epochs and best accuracy for the model was achieved at 3rd epoch for the gender task. By fine-tuning the model on gender task the model was able to produce good results for both the tasks.

### Auxiliary branches for different use-case 
In this experimentation gender and re-id tasks share the same backbone. IBN-net-a is based on resnet-18 Architecture. This architecture is mainly divided into four feature block.Initial experimentation's were conducted where both the task was sharing same feature backbone block, even though task are co-related in nature the model was failing to maintain feature exclusivity for both the tasks. As a solution, feature block for both the tasks needs to be separated, For gender classification, the 3rd feature block of the network was utilized. The output of the feature block was then given to a convolution operation which helps the model to learn exclusive features for the gender task. For Person Re-identification, the 4th feature block of the network was utilized. The diagrammatic flow of above description is mentioned below. 

![Concurrent_gender_reid drawio (1)](https://user-images.githubusercontent.com/72791642/190330969-2bbfd614-b197-434c-b7c1-73f48120b5c3.png)


### Post processing 
#### Re-ranking k-reciprocal neighbors
 When considering person re-identification (re-ID) as a retrieval process, re-ranking is a critical step to improve its accuracy.In implementation we have considered k-reciprocal encoding method to re-rank the re-ID results. The intuition behind re ranking is that if a gallery image is similar to the probe in the k-reciprocal nearest neighbors, it is more likely to be a true match. Specifically, given an image, a k-reciprocal feature is calculated by encoding its k-reciprocal nearest neighbors into a single vector, which is used for re-ranking under the Jaccard distance. The final distance is computed as the combination of the original distance and the Jaccard distance.re-ranking method  does not require any human interaction or any labelled data, so it is applicable to large-scale datasets

### 3D Modeling Projection
#### Person Re-Identification 
Below is sample video of visualization of model output and forming 3D clusters by grouping of similar appearing PID's(Person images), to visualize below result Tensorborad projector implementation has been used with t-SNE algorithm for dimensionality reduction by keeping the feature information intact, as from below video it can be seen that the clusters are forming very unique & distinct group representations with minimal anomaly for all identical appearing PID's which shows that the trained model is effectively working for the given use-case


https://user-images.githubusercontent.com/72791642/190131727-2f8557a2-96bb-476f-a692-fa49ffdf9f61.mp4


#### Gender classification 
Below is a sample video of the visualization of feature embedding of the model at the gender classification node. As per the below result, the model is able to form two clusters for male and female images. There are a few images where the model struggles to classify the person's gender; such images lie between two clusters.

https://user-images.githubusercontent.com/72791642/190126546-38ea57cc-b5ba-42a5-80cf-f1e55c1eb726.mp4


### Application utility
#### Person Re-Identification
For quick evaluation of trained Re-ID model the feature embedding are used with image retrieval problem to retrieve relevant appearing images from the set of gallery images, below are two scenarios where in first one the query image & retrieved results are close enough as in the second case the input & retrieved images are pretty distinct indicating as worst case scenario.
<table>
<th colspan="2" align='center'>
Best Case
</th>
<tr align='center'>
<td>Input Query</td>
<td>Top-k Results</td>
</tr>

<tr>
<td><img src="https://user-images.githubusercontent.com/72791642/190101434-782a23c7-5e67-4d1e-9fb1-68a496d72344.png" width="125" height="300"></td>
<td><img src="https://user-images.githubusercontent.com/72791642/190101660-ad50e56f-9ac9-479f-a46d-f6ba372a5001.png" width="300" height="700"></td>
</tr>
<tr>

<td><img src="https://user-images.githubusercontent.com/72791642/190114833-00252a8a-3b1d-44b4-a05d-c66e5a6448d4.png" width="125" height="300"></td> 
<td><img src="https://user-images.githubusercontent.com/72791642/190114931-6c1470a3-252a-43ed-87ce-5f90c436798f.png" width="300" height="700"></td> 
</tr>


<th colspan="2" align='center'>
Worst Case
</th>
<tr align='center'>
<td>Input Query</td>
<td>Top-k Results</td>
</tr>

<tr>
<td><img src="https://user-images.githubusercontent.com/72791642/190117456-51a452a4-c363-4f1b-9e74-10f6e7e3090d.png" width="125" height="300" ></td>
<td><img src="https://user-images.githubusercontent.com/72791642/190117735-efddb0c6-cd2a-49fe-aafd-8bc3249fea55.png" width="300" height="700"></td>
</tr>

<tr>
<td><img src="https://user-images.githubusercontent.com/72791642/190117911-f81f0fde-a628-4646-9e02-663ea9693fa6.png" width="125" height="300" ></td>
<td><img src="https://user-images.githubusercontent.com/72791642/190118007-1cb8de87-6f74-4c33-9492-5766417de546.png" width="300" height="700"></td>
</tr>

</table>

<h4>Gender Classification</h4>

<table>
<th colspan="2" align='center'>
Best Case
</th>
<tr align='center'>
<td>Category</td>
<td>Results</td>
</tr>

<tr>
<th>
Male
</th>
<td>
<img src="https://user-images.githubusercontent.com/72791642/190174154-ba3aea20-f3b7-4659-a2a5-ebe9defdadda.png" width="125" height="300">
<img src="https://user-images.githubusercontent.com/72791642/190174565-d56a5412-97e3-4cf9-8b70-8b71b8b2f101.png" width="125" height="300">
<img src="https://user-images.githubusercontent.com/72791642/190174723-b866cca3-7a29-4805-ba1d-39cea62ba0e0.png" width="125" height="300">
</td>
</tr>
<tr>
<th>
Female
</th> 
<td>
<img src="https://user-images.githubusercontent.com/72791642/190177182-6158ebd0-5a4a-49c4-a609-cf6ed5d06b4a.png" width="125" height="300">
<img src="https://user-images.githubusercontent.com/72791642/190177364-d0defa49-41ce-4fe0-8f46-cd61e5ac1098.png" width="125" height="300">
<img src="https://user-images.githubusercontent.com/72791642/190177562-974b2218-651e-4bb6-8462-b53fadb17960.png" width="125" height="300">
</td> 
</tr>

<th colspan="2" align='center'>
Worst Case
</th>
<tr align='center'>
<td>Category</td>
<td>Results</td>
</tr>

<tr>
<th>
Male
</th>
<td>
<img src="https://user-images.githubusercontent.com/72791642/190178015-11e92fbb-325e-4a7f-b401-38a54120e7c8.png" width="125" height="300">
<img src="https://user-images.githubusercontent.com/72791642/190178366-c1421e86-11bf-4e20-bca1-3f0c0375bb46.png" width="125" height="300">
<img src="https://user-images.githubusercontent.com/72791642/190178306-765e41d5-e048-4ad9-bbaa-6b2eb7775da0.png" width="125" height="300">
</td>
</tr>


<tr>
<th>
Female
</th> 
<td>
<img src="https://user-images.githubusercontent.com/72791642/190178699-9deb75a4-f350-4f2f-806e-404fff5cedd3.png" width="125" height="300">
<img src="https://user-images.githubusercontent.com/72791642/190178751-f600c05e-c968-4b81-87b2-2299d6c17f68.png" width="125" height="300">
<img src="https://user-images.githubusercontent.com/72791642/190178765-d1b03848-2639-488d-b829-6399d959e819.png" width="125" height="300">
</td> 
</tr>
</table>


## Evaluation Metrics
As this model is trained on two use-cases, below are the results for the Gender and Person Re-identification tasks.
### Gender Classification 
Evaluation Dataset used for Testing is PR [#278](https://github.com/onearmbandit/VAS-Data-Science/pull/278), [#279](https://github.com/onearmbandit/VAS-Data-Science/pull/279) and M.A.R [v0.1-alpha](https://github.com/onearmbandit/VAS-Data-Science/releases/tag/v0.1-alpha)
| Categories      | precision | recall     | f1-score    | support  |
| :---        |    :----:   |          ---: |          ---:|          ---:|
| male      | 0.89       | 0.89   | 0.89   | 8170   |
| Female   | 0.79        | 0.80    | 0.80    | 4442    |
| Accuracy   |         |     | 0.86   | 12612    |
| macro avg   | 0.84    | 0.85    | 0.84   | 12612    |
| weighted avg   | 0.86    | 0.86    | 0.86   | 12612    |

### Person Re-Identification 
Person re-identification is the task of associating images of the same person taken from different cameras or from the same camera on different occasions. In the current release, we will be including benchmark accuracy for single camera and multi-camera views.
#### Market-1501 Dataset (Benchmark dataset)
For person re identification the Benchmark accuracy we have considered the Market-1501 dataset. The dataset is split into two parts: 750 identities are utilized for training and the remaining 751 identities are used for testing. In the official testing protocol 3,368 query images are selected as a probe set to find the correct match across 19,732 reference gallery images.
<table>
<th colspan="4" align='center'>
without re-ranking 
</th>
<tr>
<th colspan="2" align='center'>
single camera
</th>
<th colspan="2" align='center'>
multi-camera camera
</th>
</tr>
<tr>
<th style="width:70%">Top-K</th>
<th>Accuracy</th>
<th style="width:70%">Top-K</th>
<th>Accuracy</th>
</tr>
<tr>
<td>Rank-1</td>
<td>99.6%</td>
<td>Rank-1</td>
<td>87.6%</td>
</tr>
<tr>
<td>Rank-5</td>
<td>99.8%</td>
<td>Rank-5</td>
<td>95.7%</td>
</tr>
<tr>
<td>Rank-10</td>
<td>99.8%</td>
<td>Rank-10</td>
<td>97.4%</td>
</tr>
<tr>
<td>Rank-20</td>
<td>99.9%</td>
<td>Rank-20</td>
<td>98.2%</td>
</tr>
<tr>
<td>Rank-50</td>
<td>99.9%</td>
<td>Rank-50</td>
<td>99.0%</td>
</tr>
<tr>
<td>mAP</td>
<td>0.79</td>
<td>mAP</td>
<td>0.73</td>
</tr>
<tr>
<th colspan="4" align='center'>
with re-ranking 
</th>
</tr>
<tr>
<th colspan="2" align='center'>
single camera
</th>
<th colspan="2" align='center'>
multi-camera camera
</th>
</tr>
<tr>
<th style="width:70%">Top-K</th>
<th>Accuracy</th>
<th style="width:70%">Top-K</th>
<th>Accuracy</th>
</tr>
<tr>
<td>Rank-1</td>
<td>98.3%</td>
<td>Rank-1</td>
<td>90.1%</td>
</tr>
<tr>
<td>Rank-5</td>
<td>99.3%</td>
<td>Rank-5</td>
<td>94.3%</td>
</tr>
<tr>
<td>Rank-10</td>
<td>99.6%</td>
<td>Rank-10</td>
<td>95.5%</td>
</tr>
<tr>
<td>Rank-20</td>
<td>99.8%</td>
<td>Rank-20</td>
<td>97.1%</td>
</tr>
<tr>
<td>Rank-50</td>
<td>99.9%</td>
<td>Rank-50</td>
<td>98.6%</td>
</tr>
<tr>
<td>mAP</td>
<td>0.89</td>
<td>mAP</td>
<td>0.86</td>
</tr>

</table>

<h3>Real time testing Earthcam USA dataset</h3>
This Dataset was collected from live feed  from a  earthcam.com therefore it contains only a single camera instance. This dataset contains over 2500+ person ID, which helps us to get estimates of model performance in real-time scenario. 

<table>
<th colspan="2" align='center'>
without re-ranking 
</th>
<th colspan="2" align='center'>
with re-ranking 
</th>
<tr>
<th colspan="2" align='center'>
single camera
</th>
<th colspan="2" align='center'>
single camera
</th>
</tr>
<tr>
<th style="width:70%">Top-K</th>
<th>Accuracy</th>
<th style="width:70%">Top-K</th>
<th>Accuracy</th>
</tr>
<tr>
<td>Rank-1</td>
<td>79.3%</td>
<td>Rank-1</td>
<td>98.4%</td>
</tr>
<tr>
<td>Rank-5</td>
<td>88.4%</td>
<td>Rank-5</td>
<td>99.3%</td>
</tr>
<tr>
<td>Rank-10</td>
<td>90.9%</td>
<td>Rank-10</td>
<td>99.6%</td>
</tr>
<tr>
<td>Rank-20</td>
<td>92.6%</td>
<td>Rank-20</td>
<td>99.8%</td>
</tr>
<tr>
<td>Rank-50</td>
<td>94.7%</td>
<td>Rank-50</td>
<td>99.9%</td>
</tr>
<tr>
<td>mAP</td>
<td>0.51%</td>
<td>mAP</td>
<td>0.89</td>
</tr>
</table>

## Conclusive Remark
All the experimentation's conducted in PR https://github.com/onearmbandit/VAS-Core-Research/pull/73 were primarily intended for optimizing the end-to-end NN architecture by unifying the backbone network for multi-task use-cases, the feature dimensions reduced from 512-d to 128-d which optimizes the memory footprints resulting in space complexity optimization. As of now some part of the dataset for this problem is still in-progress, in order to get the desired accuracy numbers for this problem more real-time dataset needs to be prepared to evaluate it with existing as well as upcoming experimentation results respectively, all the results mentioned in the doc [here](https://docs.google.com/document/d/1OMxwlCedrsxOHhfWJDdXBUI44zozGDlpwGGIOMNAa_A/edit#heading=h.vp68dp1vb7pp) are for testing and experimentation purpose only, the real-time accuracy numbers mentioned in the evaluation metric section signifies to the dataset collected from live video feeds available with single camera view, however the results looks more promising but these results need to be validated with respect to end-to-end forensic search based use-case implementation.  

## Known Issues
- As mentioned above model performs well on training and validation dataset but when tested on real time dataset accuracy of the model drops significantly
- Lack of discriminative inter-class features makes gender classification difficult
- Full body appearance makes hard for classification model to correctly classify between male & female class.
- Limited number of data samples
- The potential risk of model over-fitting because of the common identity issue in the dataset.
- As the ReID network is cascaded along with detection model, the cascading effect adds significant toll in latency, which increases end-to-end FPS & latency numbers.
- Model is not trained with Reviewed Dataset, however diversified source dataset is used for training & validation.

## Further improvements (#TODO):
- Utilizing more amount of discrete dataset, to further improve model accuracy and minimizing the risk of over-fitting.
- Fine-tuning gender classification use-case on top of unsupervised learning approach as per PR #71
- Addressing the gender classification using facial attribute problem instead of full body R.O.I as input.
- Implementing FairMOT based architecture for training and inference to eliminate the cascading effect caused by detection model followed by ReID model.
- NN Quantization based optimizations to reduce infer latency numbers.
- Post-processing optimization to reduce infer latency numbers
- Implementation of Mobile one architecture #78 
- implementation of GPU efficient architecture #76 
- Implementation of Multi-attribute classification using Person re-identification 
