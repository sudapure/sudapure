# TensorRT Release for full-body Gender Classification based on ResNet-IBN-18-a Architecture
This release contains baseline TensorRT version of IBN-Net(ResNet-18-a-aux) backbone architecture to solve full-body person gender classification problem as covered in PR https://github.com/onearmbandit/VAS-Video-Analytics/pull/108, this implementation primarily contains TensorRT based deployment release for research release [v1.6-beta](https://github.com/onearmbandit/VAS-Core-Research/releases/tag/v1.6-beta), this release was supposed to be published on 21/11/2022 but due to PR reviews & other priority tasks it got delayed, for detailed varying combination of baseline research experimentation's for this implementation carried in release [v1.6-beta](https://github.com/onearmbandit/VAS-Core-Research/releases/tag/v1.6-beta) and documented [here](https://docs.google.com/document/d/1OMxwlCedrsxOHhfWJDdXBUI44zozGDlpwGGIOMNAa_A/edit#heading=h.rwubogtek0go).

## Change Logs
* baseline implementation of SOT based on stark NN architecture by @sudapure in https://github.com/onearmbandit/VAS-Video-Analytics/pull/101
* TRT Network implementation of Concurrent Gender & ReID use-case by @sudapure in https://github.com/onearmbandit/VAS-Video-Analytics/pull/108

## Improvements
-  Exporting the PyTorch trained weights to intermediate key/value pair representation :heavy_check_mark::1st_place_medal: 
-  DVC integration for versioning all weight/engine files. :heavy_check_mark::1st_place_medal: 
-  Implementing backbone Network architecture using TRT based Network Builder API. :heavy_check_mark::1st_place_medal: 
-  Implementing gender auxiliary branch Network architecture using TRT based Network Builder API. :heavy_check_mark::1st_place_medal: 
- Building quantize engine weights using INT8 network definition as per PR https://github.com/onearmbandit/VAS-Video-Analytics/pull/75. :heavy_check_mark::1st_place_medal: 
-  Cross-validating the latency/accuracy benchmarks of INT8 precision with FP32 & FP16 precision mode. :heavy_check_mark::1st_place_medal: 
- TensorRT engine serialization support for FP32/FP16/INT8(PTQ) :heavy_check_mark::1st_place_medal: 

## Specification
<table>
<tr>
<th>Metric</th>
<th>Value</th>
</tr>
<tr>
<td>f1-score accuracy</td>
<td>80</td>
</tr>
<tr>
<td>Pose coverage</td>
<td>Standing upright, parallel to image plane</td>
</tr>
<tr>
<td>Camera Angle</td>
<td><= &ang;40&#176;</td>
</tr>
<tr>
<td>Min object width</td>
<td>80 pixels</td>
</tr>
<tr>
<td>Support of occluded pedestrians </td>
<td>Yes</td>
</tr>
<tr>
<td>Occlusion coverage  </td>
<td> <20% </td>
</tr>
<tr>
<td>GFlops  </td>
<td>N/A </td>
</tr>
<tr>
<td>MParams   </td>
<td>N/A </td>
</tr>
<tr>
<td>Source framework </td>
<td>PyTorch*</td>
</tr>
</table>

## Implementation
As per research release [v1.6-beta](https://github.com/onearmbandit/VAS-Core-Research/releases/tag/v1.6-beta) it's primary objective is to unify the backbone network to solve ReID and Gender classification use-case, however for this release only gender auxiliary branch from the backbone network along with head network has implemented. IBN-Net is a novel convolutional architecture built upon the foundation of the ResNet-18 architecture. IBN-net uses Instance Batch normalization(IN) with a combination of vanilla batch normalization to produce high generalizable features for a given input query. Below is the architecture for ResNet-18, IBN-net-A and IBN-net-B.

### Network Architecture
IBN-net-A seems to perform better than other experiments with respect to inference cost and accuracy. Therefore this release focuses on IBN-Net-A architecture with gender auxiliary branch for gender classification use-case.
<p align='center'>
<img src="https://user-images.githubusercontent.com/57352045/148694823-025b41dc-4759-447a-8d4b-87c89ea3a164.png"/>
</p>

### DataSet Used
We use two independent dataset for both the task. For person re-identification PR https://github.com/onearmbandit/VAS-Data-Science/pull/264 for gender task PR https://github.com/onearmbandit/VAS-Data-Science/pull/278 https://github.com/onearmbandit/VAS-Data-Science/pull/279 and M.A.R [v0.1-alpha](https://github.com/onearmbandit/VAS-Data-Science/releases/tag/v0.1-alpha) was used for training and testing. Both the dataset mentioned above are independent and do not share common images or annotation.The data-loader was designed to pass the task specific batches in random pattern such that both task are learned concurrently.

### Model Summary
Below is the intermediate network model summary after exporting the pre-trained pytorch weights to intermediate key,value format. Below network summary only contains gender auxiliary backbone network along with classification head.
```
GenderModel(
  (backbone): ResNetIBNAux(
    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
    (layer1): Sequential(
      (0): BasicBlock_IBN(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): IBN(
          (IN): InstanceNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          (BN): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): BasicBlock_IBN(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): IBN(
          (IN): InstanceNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          (BN): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (layer2): Sequential(
      (0): BasicBlock_IBN(
        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn1): IBN(
          (IN): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          (BN): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (downsample): Sequential(
          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BasicBlock_IBN(
        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): IBN(
          (IN): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          (BN): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (layer3): Sequential(
      (0): BasicBlock_IBN(
        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn1): IBN(
          (IN): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          (BN): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (downsample): Sequential(
          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BasicBlock_IBN(
        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): IBN(
          (IN): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          (BN): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (gender_classifier): Classifier(
    (linear_layers): Linear(
      (linear): Sequential(
        (0): Linear(in_features=512, out_features=128, bias=True)
        (1): Linear(in_features=128, out_features=1, bias=True)
      )
    )
  )
  (gender_conv): Sequential(
    (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): AvgPool2d(kernel_size=7, stride=7, padding=0)
  )
)
```
## Architecture Design
### Auxiliary branches for different use-case
In the baseline implementation gender and re-id tasks share the same backbone. IBN-net-a is based on resnet-18 Architecture. This architecture is mainly divided into four feature block.Initial experimentation's were conducted where both the task was sharing same feature backbone block, even though task are co-related in nature the model was failing to maintain feature exclusivity for both the tasks. As a solution, feature block for both the tasks needs to be separated, For gender classification, the 3rd feature block of the network was utilized. The output of the feature block was then given to a convolution operation which helps the model to learn exclusive features for the gender task. For Person Re-identification, the 4th feature block of the network was utilized. The diagrammatic flow of above description is mentioned below.

<img src="https://user-images.githubusercontent.com/72791642/190330969-2bbfd614-b197-434c-b7c1-73f48120b5c3.png"/>

## Application Utility
### Gender Classification
For quick evaluation of trained  model the classifier output is used with some selected input images to classify the images as Male or Female, below are two scenarios where in first one the query image & retrieved results are close enough as in the second case the input & classified images are pretty distinct indicating as worst case scenario.

<table>
<th colspan="2" align='center'>
Best Case
</th>
<tr align='center'>
<td>Category</td>
<td>Results</td>
</tr>

<tr>
<th>
Male
</th>
<td>
<img src="https://user-images.githubusercontent.com/72791642/190174154-ba3aea20-f3b7-4659-a2a5-ebe9defdadda.png" width="125" height="300">
<img src="https://user-images.githubusercontent.com/72791642/190174565-d56a5412-97e3-4cf9-8b70-8b71b8b2f101.png" width="125" height="300">
<img src="https://user-images.githubusercontent.com/72791642/190174723-b866cca3-7a29-4805-ba1d-39cea62ba0e0.png" width="125" height="300">
</td>
</tr>
<tr>
<th>
Female
</th> 
<td>
<img src="https://user-images.githubusercontent.com/72791642/190177182-6158ebd0-5a4a-49c4-a609-cf6ed5d06b4a.png" width="125" height="300">
<img src="https://user-images.githubusercontent.com/72791642/190177364-d0defa49-41ce-4fe0-8f46-cd61e5ac1098.png" width="125" height="300">
<img src="https://user-images.githubusercontent.com/72791642/190177562-974b2218-651e-4bb6-8462-b53fadb17960.png" width="125" height="300">
</td> 
</tr>

<th colspan="2" align='center'>
Worst Case
</th>
<tr align='center'>
<td>Category</td>
<td>Results</td>
</tr>

<tr>
<th>
Male
</th>
<td>
<img src="https://user-images.githubusercontent.com/72791642/190178015-11e92fbb-325e-4a7f-b401-38a54120e7c8.png" width="125" height="300">
<img src="https://user-images.githubusercontent.com/72791642/190178366-c1421e86-11bf-4e20-bca1-3f0c0375bb46.png" width="125" height="300">
<img src="https://user-images.githubusercontent.com/72791642/190178306-765e41d5-e048-4ad9-bbaa-6b2eb7775da0.png" width="125" height="300">
</td>
</tr>

<tr>
<th>
Female
</th> 
<td>
<img src="https://user-images.githubusercontent.com/72791642/190178699-9deb75a4-f350-4f2f-806e-404fff5cedd3.png" width="125" height="300">
<img src="https://user-images.githubusercontent.com/72791642/190178751-f600c05e-c968-4b81-87b2-2299d6c17f68.png" width="125" height="300">
<img src="https://user-images.githubusercontent.com/72791642/190178765-d1b03848-2639-488d-b829-6399d959e819.png" width="125" height="300">
</td> 
</tr>
</table>

## Evaluation Metrics
As primary objective of this release implementation is to achieve similar output tensor precision as that of PyTorch based implementation, output tensor after inferring the input using PyTorch model in Python,
inferred the TRT generated model across dataset available in [PR#278](https://github.com/onearmbandit/VAS-Data-Science/pull/278) and compared the results with Pytorch as mentioned in the Evaluation Metrics section of release [v1.6-beta](https://github.com/onearmbandit/VAS-Core-Research/releases/tag/v1.6-beta)
```
predict csv: Enter CSV_Path
/home/swap/cemtrex/Video-Analytics/cpp_research/ibn_trt_cpp/build/predictions.csv

==================================== inference-stats =====================================

              precision    recall  f1-score   support

        male       0.92      0.91      0.91      5975
      female       0.87      0.88      0.87      3908

    accuracy                           0.90      9883
   macro avg       0.89      0.89      0.89      9883
weighted avg       0.90      0.90      0.90      9883

```
the results for dataset after reproducing the PyTorch model across TensorRT model are almost identical!

As per the [PR#75](https://github.com/onearmbandit/VAS-Video-Analytics/pull/75#issue-1216863984), the required calibration dataset for INT8 inference has been pulled from PR [#256](https://github.com/onearmbandit/VAS-Data-Science/pull/256) & [#262](https://github.com/onearmbandit/VAS-Data-Science/pull/262) from [onearmbandit/VAS-Data-Science](https://github.com/onearmbandit/VAS-Data-Science) repo, ideally the calibration dataset is assumed to be subset of test/val dataset, the underlying dataset in mentioned PR's could be referred for test/val purpose while training the gender classification or ReID model, also dataset required for validating the accuracy benchmarks of INT8 precision across FP32 has been conducted as two experiments and it's mentioned in the below table, below are accuracy numbers observed for inferring the same gender classification implementation from research(PyTorch) release [v1.6-beta](https://github.com/onearmbandit/VAS-Core-Research/releases/tag/v1.6-beta) model for INT8 & FP32 precision's,

<table>
<tr>
<td>Use-Case</td>
<td>Val Dataset</td>
<td>Framework/IE</td>
<td>Architecture(GPU/CPU)</td>
<td>Infer Resolution</td>
<td>Accuracy/f1-score</td>
<td>Latency</td>
<td>Batch Size</td>
<td>Precision Mode</td>

</tr>

<tr>
<td>Gender Net</td>
<td><a href="https://github.com/onearmbandit/VAS-Data-Science/pull/327">PR327</a> + <a href="https://github.com/onearmbandit/VAS-Data-Science/pull/373">PR373</a></td>
<td>TensorRT</td>
<td>Nvidia GTX-1060</td>
<td>224x224</td>
<td>0.73(M)/0.82(F)/0.78</td>
<td>1.599177 ms</td>
<td>1</td>
<td>FP32</td>

</tr>

<tr>
<td>Gender Net</td>
<td><a href="https://github.com/onearmbandit/VAS-Data-Science/pull/327">PR327</a> + <a href="https://github.com/onearmbandit/VAS-Data-Science/pull/373">PR373</a></td>
<td>TensorRT</td>
<td>Nvidia GTX-1060</td>
<td>224x224</td>
<td>0.74(M)/0.81(F)/0.78</td>
<td>1.170507 ms</td>
<td>1</td>
<td>INT8</td>

</tr>


<tr>
<td>Gender Net</td>
<td><a href="https://github.com/onearmbandit/VAS-Data-Science/pull/278">PR278</a></td>
<td>TensorRT</td>
<td>Nvidia GTX-1060</td>
<td>224x224</td>
<td>0.91(M)/0.87(F)/0.90</td>
<td>1.599177 ms</td>
<td>1</td>
<td>FP32</td>

</tr>

<tr>
<td>Gender Net</td>
<td><a href="https://github.com/onearmbandit/VAS-Data-Science/pull/278">PR278</a></td>
<td>TensorRT</td>
<td>Nvidia GTX-1060</td>
<td>224x224</td>
<td>0.91(M)/0.87(F)/0.90</td>
<td>1.194035 ms</td>
<td>1</td>
<td>INT8</td>

</tr>

</table>

As from above table, there is slight improvement in latency number for inferring in INT8 mode on GTX 1060 card these number could improve even better on RTX-30xx series cards which will be evaluated soon,


**NOTE**: The calibration dataset used for above results are  contextually relevant but might not be sufficiently enough to produce near similar results to FP32 precision. As a matter of fact improving accuracy for INT8 mode is entirely possible by more advanced method i.e. QAT(quantization aware training) which will be addressed as a separate optimization task.

_Above results might entirely depend on the test/val dataset used for accuracy validation, however validating the model across different modality dataset's for contextually similar calibration dataset could produce significantly distinct results. As per recommendations it is required to have significantly large subset of calibration dataset for generating INT8 models, as new dataset PR's getting added these results will be getting reproduced accordingly, but as per observations INT8 generated results are almost identical with FP32 precision numbers_

## Conclusive Remark
The primary purpose of this release is to export the `IBN-Net-Resnet-18-a` architecture to intermediate format, and validating the accuracy benchmarks with respect to the baseline implementation as already implemented in PyTorch, as done in [this](https://github.com/onearmbandit/VAS-Core-Research/pull/108) PR. The expected outcome of this release is getting the near similar benchmark number for accuracy by optimizing the infer latency using TensorRT specific implementation for as that of the research based PyTorch implementation as documented [here](https://docs.google.com/document/d/1rnSRnbJQLRgfR3DjcOuHTVHA51f_Q8tPZpK0H-t5ep0/edit#heading=h.of82xekw4fe).
The infer latency has been emphasized for this release as a part of deployment phase,  the accuracy numbers are subject to training & research phase as covered in release [v1.6-beta](https://github.com/onearmbandit/VAS-Core-Research/releases/tag/v1.6-beta) onearmbandit/VAS-Core-Research.

## Known Issues
- As per observations model performs well on training and validation dataset but when tested on real time dataset accuracy of the model drops significantly.
- Lack of discriminative inter-class features makes gender classification difficult
- Full body appearance makes hard for classification model to correctly classify between male & female class.
- The potential risk of model over-fitting because of the common identity issue in the dataset.
- Model produces false results for input image below certain A.R.
- As the end-to-end network is cascaded along with detection model, the cascading effect adds significant toll in latency, which increases end-to-end FPS & latency numbers.
- Model is not trained with Reviewed Dataset, however diversified source dataset is used for training & validation.

## Further improvements (#TODO):
- Improving accuracy without TTA, which is crucial for deployment specific releases.
- NN architecture optimization & decreasing trainable parameter to further improve latency numbers
- Implementation of custom NN operators to optimize infer latency by keeping accuracy intact.
- Implementation of QAT(Quantization Aware Training) for optimizing infer latency using INT8 Quantization mode.
