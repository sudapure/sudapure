# Traffic Light Classification & Blinking State Detection Release
This release contains baseline implementation of trafficNet architecture and various experimentation's conducted on it including Grad-CAM visualization , the primary objective is to classify the traffic light as red, green, yellow, no_light or as blinking state, the experimentation's covered in this release signifies to preliminary implementation of traffic classification problem using CNN along with other functionalities covered in the subsequent PR's.

## Change Log
* PR for traffic light demo, and sort tracking algorithm by @sudapure in https://github.com/onearmbandit/VAS-Core-Research/pull/6
* finetune resenet18 by @patricia-rod in https://github.com/onearmbandit/VAS-Core-Research/pull/15
* new architecture by @patricia-rod in https://github.com/onearmbandit/VAS-Core-Research/pull/19
* refactor as per alpha design issue #22 by @sudapure in https://github.com/onearmbandit/VAS-Core-Research/pull/23
* Traffic light network by @sarveshamberkarC in https://github.com/onearmbandit/VAS-Core-Research/pull/46
* Traffic Light bug fixes  by @sarveshamberkarC in https://github.com/onearmbandit/VAS-Core-Research/pull/53
* Traffic blinking light by @patricia-rod in https://github.com/onearmbandit/VAS-Core-Research/pull/55
* For issues 26-27-28 by @patricia-rod in https://github.com/onearmbandit/VAS-Core-Research/pull/32
* Issue 28 by @patricia-rod in https://github.com/onearmbandit/VAS-Core-Research/pull/36

### Issues Resolved

- [Add class for no_light in training phase (new dataset)](https://github.com/onearmbandit/VAS-Core-Research/issues/2) :heavy_check_mark: :1st_place_medal: 
- [New architecture version for TrafficNet](https://github.com/onearmbandit/VAS-Core-Research/issues/17) :heavy_check_mark: :1st_place_medal: 
- [Improving the Real - Time Accuracy of Traffic Light Network](https://github.com/onearmbandit/VAS-Core-Research/issues/26) :heavy_check_mark: :1st_place_medal: 
- [Detecting traffic light blinking state for traffic light classification model](https://github.com/onearmbandit/VAS-Core-Research/issues/52) :heavy_check_mark: :1st_place_medal: 

### New Contributors
* @kparidacemtrex made their first contribution in https://github.com/onearmbandit/VAS-Core-Research/pull/1
* @sudapure made their first contribution in https://github.com/onearmbandit/VAS-Core-Research/pull/6

**Full Changelog**: https://github.com/onearmbandit/VAS-Core-Research/commits/v1.3-beta

## Dataset Used
All the experimentation's conducted for this release has been done on [PR#80](https://github.com/onearmbandit/VAS-Data-Science/pull/80) traffic dataset release.
<table>
<tr>
<td>
<img src="https://user-images.githubusercontent.com/57352045/148762978-4701f1f8-6ed8-443c-a30b-d7cbabfdfbfb.png"/></td>
<td>
<img src="https://user-images.githubusercontent.com/57352045/148762956-a87dff0f-afbb-4e36-95ec-62233de799ca.png"/></td>
</tr>
<tr>
<td align='center'>traffic light imbalance</td>
<td align='center'>color space graph for green traffic light dataset</td>
</tr>

<tr>
<td>
<img src="https://user-images.githubusercontent.com/57352045/148762962-40b19f76-a504-4d0a-a00c-f5affd2f108c.png"/></td>
<td>
<img src="https://user-images.githubusercontent.com/57352045/148762971-1f68e355-d40f-4008-bf86-c3197e60c045.png"/></td>
</tr>
<tr>
<td align='center'>color space graph for red traffic light dataset</td>
<td align='center'>color space graph for yellow traffic light dataset</td>
</tr>
</table>

**NOTE**: _**The number of data samples showed in above graphs are only from the training datasets. From the above graph the stats for no_light is not available as the E.D.A report wasn't updated after some time, for more details on dataset refer [PR#80](https://github.com/onearmbandit/VAS-Data-Science/pull/80)**_.

## Experimentation Details
### Network Architecture
The TrafficNet architecture Comprises of three standard convolution blocks with Max- Pooling and Batchnorm and activation function as relu. After each convolution block Squeeze-and-Excitation Block is used except the last convolution block. The Squeeze-and-Excitation Block is an architectural unit designed to improve the representational power of a network by enabling it to perform dynamic channel-wise feature re-calibration.This is followed by Linear configuration. Total trainable parameter in the model are 260483.

Optimizer used is stochastic gradient descent with base learning rate of 0.01 and momentum 0.9.L2 normalization was used to prevent model from overfitting.To prevent overfitting a custom manual scheduler was designed which takes new learning rate from user if the validation loss has plateaued.

### Implementation
Model was trained for five epochs, after each training batch model was validated on validation data and the model with minimum validation loss value was saved and minimum loss was achieved on the 1st epoch. As the data was highly imbalanced we used Weighted Random Sampler which will oversample the class containing less images and under-sample the class containing more images. In order to debug the model we used grad-cam while training to verify generalization of the model for further details you can find training logs [here](https://drive.google.com/file/d/1lFeH9FP5Ggrn2VX6KFq0D-W5JDWqA47G/view?usp=sharing).Below results are taken with TTA(Test Time Augmentation) however it is observed that without TTA accuracy drops drastically,

<table>
<tr>
<td colspan="2" align='center'>
<img src="https://user-images.githubusercontent.com/57352045/148765092-f4032f2d-b0c2-4664-ad5c-39981114aa3c.png"/></td>
</tr>
<tr>
<td colspan="2" align='center'>Day-time performance</td>
</tr>

<tr>
<td>
<img src="https://user-images.githubusercontent.com/57352045/148766491-ef53532e-a587-4522-9015-fe87cfb74051.png"/></td>
<td>
<img src="https://user-images.githubusercontent.com/57352045/148767299-11720860-89c4-4598-a7a7-ed91d2b11fb2.png"/></td>
</tr>
<tr>
<td align='center'>Night-time performance</td>
<td align='center'>Early-morning performance</td>
</tr>
</table>

### Resolving Blinking State Issue
As functional implementation of traffic light classification in issue #26, the network can classify the traffic lights in four states red,green,yellow,no_light , however the no_light/blank state was introduced primarily for eliminating the false classification produced in case of blinking light state, another reason is when the no light is active, however as per the experimentation results in the report after training the network with four classes the accuracy has dropped significantly as compared with the three classes, this happen because of the imbalanced dataset, but as per the observations and inferring the models over some real-time videos the blinking light condition is very often and can't be avoided, while trying different business logic to address the blinking state problem following two approaches has been implemented,

1. In [this](https://github.com/onearmbandit/VAS-Core-Research/pull/55/commits/97a5111034d5bed7994e6fcf7865c20d850ee32f) commit  appending light states in a list with size = fps. If the size is greater than the current fps we are popping the first element from the list. we are just lopping through a list to check current and next by checking one of the states is blank. if they aren't equal we are stating it's a blinking state.
for e.g. if we are having this list `['blank','blank','blank','blank','blank','red','red','red','red','red']` then we are checking this condition from buffer.`['blank','red']`

2. In [this](https://github.com/onearmbandit/VAS-Core-Research/pull/55/commits/09fee29cb72fca596eee24e604c01496d171ab0b) approach instead of taking aggregation we are taking count of yellow & blank, red & blank, green & blank, and checking if it's greater than 1/4 of fps. Since fps can be varied taking aggregation can be computationally expensive because aggregation also will be taking count of each frame and generating key-frames and then predicting the final output and the response from this method can be a bit slower.

<table>
<tr>
<td>
<img height=400 width=600 src='https://user-images.githubusercontent.com/57352045/145561116-bfa382c6-5704-4775-9014-2b292307c00a.gif'/>
</td>
<td>
<img height=400 width=600 src='https://user-images.githubusercontent.com/72556313/147724284-c5dc5f5a-2b30-471e-91d6-dbee6647b82e.gif'/>
</td>
</tr>
<tr>
<td align='center'>Without Blink Detection</td>
<td align='center'>With Blink Detection</td>
</tr>
</table>

## Conclusive Remark
As per Observations(on real-time videos) accuracy of the model on early morning and late night data is good, as color saturation of traffic light is adequate and environment exposure affecting traffic light?s color saturation is also low. In the day-time video [here](https://drive.google.com/file/d/1PVJvz8Xl-5ELju9DRKmaqPVf-579R8d5/view?usp=sharing) due to daylight exposure the model is not able to classify traffic lights correctly also, video resolution of real time sample video is low due to which after cropping the traffic light the resultant cropped image is of low resolution which also affects the accuracy of the model. **_The sample demo/P.O.C implementation of traffic blinking state is a subject to use-case level implementation and could potentially be consumed/referred by Product Team, the P.O.C for blink detection will not be provided as part of Core Production Lib_**.

## known Issues
- Accuracy depends on cropping the region of interest of traffic light. If the background acquires considerable area than the region of interest(traffic light) then it will have an effect on accuracy.
- Without TTA accuracy drops drastically, this is very critical issue as in deployment release TTA is not required.
- As per the observations model outputs false results particularly in case of overexposed light conditions.
- Model also depends on the light intensity of the traffic light and camera placement.
- Distance between camera and traffic light is also important factor to consider the accuracy numbers.
- Quality of the video sample on which the model is being tested.
- The model is performing well on traffic signals which are vertical in shape, as there is not enough traffic data for horizontal or any other modern shaped traffic signals , so accuracy for horizontal or other shape signals is very poor.
- For traffic blink detection there could be few corner cases which have not been tested yet, and are completely subject to product team deployment specific scenarios.
- Traffic blink detection implementation has been made robust with respect to varying FPS, but it's not been tested on distinct input streams with varying FPS.

## Further improvements (#TODO):
- Augmentation can be added using albumentations or similar libraries to augment different seasonal scenarios and light conditions for domain adaptation.
- Improving accuracy without TTA, which is crucial for deployment specific releases.
- Decreasing trainable parameter to further improve latency numbers
- Training model on blank traffic light to predict blinking traffic light.
- Model is not tested on seasonal conditions like rain and snow which might affect its performance.
- More robust buiseness logic approach can be implemented for traffic blink detection problem.


## Evaluation Metrics

<table>
<tr>
<td>Version</td>
<td>Use-Case</td>
<td>Framework/IE</td>
<td>Architecture(GPU/CPU)</td>
<td>Algorithm</td>
<td>Accuracy</td>
<td>latency</td>
<td>FPS</td>
</tr>

<tr>
<td>v1.3-beta</td>
<td>Traffic Classification</td>
<td>PyTorch</td>
<td>Nvidia GTX 1060</td>
<td>Custom TrafficNet</td>
<td>82</td>
<td>1.72 ms</td>
<td>N/A</td>
</tr>
<tr>
<td>v1.3-beta</td>
<td>Blink Detection</td>
<td>Python/NumPy</td>
<td>Intel CPU</td>
<td>Custom Logic</td>
<td>N/A</td>
<td>~1 ms</td>
<td>N/A</td>
</tr>

</table>

** All AP(f1 score) numbers are for single-model single-scale, single-channel without ensemble or test-time augmentation. Accuracy here we referring to is based on standard f1-score evaluation metric with TTA, which is based on confusion matrix, precision and recall values. for accuracy validations we are considering confusion matrix parameters with IOU threshold >= 0.5 for detection models only, for classification model IOU is not needed.

** Latency<sub>GPU/CPU</sub> measures end-to-end latency per image/frame averaged over test, validation input sources which can be from RTSP stream or video sources. using specified CPU/GPU architectures, and includes image preprocessing, inference at batch size 1, post-processing and NMS. 

**NOTE**: _The accuracy numbers obtained from above experiment may looks prominent but important thing to note is, the dataset on which the model has trained on has lot of similar appearing image samples which is been described as common identity problem in the release notes of the dataset, so there is high chance that the model got over-fit and potentially may not perform well in real-time scenarios, however the baseline implementation of trafficNet has been tested on few real-time video samples and performed well on it_.
