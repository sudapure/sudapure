# Multi Attribute Recognition Release based on IBN-Net architecture for Person & Vehicle UseCase
This release contains baseline implementation of IBN-Net, 2CNN, 3CNN, OS-Net architecture as covered in PR #47, this release was supposed to be published on 07/12/2021 but due to PR related issues(merge conflicts) it got delayed, the experimentation's covered in this release signifies to preliminary implementation for multi attribute recognition problem.

## Change Logs
- Implemented dataloader, training, model architecture, loss module for IBN-Net, 2CNN, 3CNN, OS-Net.
- Experimentation and evaluation of various M.A.R architectures(IBN-Net,2CNN,3CNN,OS-Net) on vehicle dataset from [PR#68](https://github.com/onearmbandit/VAS-Data-Science/pull/68) & documented the same [here](https://docs.google.com/document/d/1Y489flM5FKmuf5Km6kfYas7cH-tN99suoicW0XZnSDc/edit#heading=h.t8utdu7xwogx).
- Experimentation and evaluation of various M.A.R architectures(IBN-Net,2CNN,3CNN,OS-Net) on person dataset from [v0.1-alpha](https://github.com/onearmbandit/VAS-Data-Science/tree/v0.1-alpha) & documented the same [here](https://docs.google.com/document/d/1rnSRnbJQLRgfR3DjcOuHTVHA51f_Q8tPZpK0H-t5ep0/edit#heading=h.of82xekw4fe).
- Profiling and Grad-Cam visualization of BN-Inception(originally used in iccv19) architecture, [refer](https://github.com/onearmbandit/VAS-Core-Research/pull/47#issuecomment-963989194).
- Refactor dataloader, train, loss modules for PyTorch-lighting based implementation, [refer](https://github.com/onearmbandit/VAS-Core-Research/pull/47#issuecomment-970175643).
- Tensorboard integration for real-time parameter visualization, [refer](https://github.com/onearmbandit/VAS-Core-Research/pull/47#issuecomment-973983264).
- Fixing of PyTorch-lighting execution issue on Linux server for standard user(non-root), [refer](https://github.com/onearmbandit/VAS-Core-Research/pull/47#issuecomment-983393506).
- Experimentation and evaluation of various OSNet variant architecture to experiment with person/vehicle M.A.R problem, [refer](https://github.com/onearmbandit/VAS-Core-Research/pull/47#issuecomment-986685769).
- Integration of DVC for model versioning and experimentation & re-produced the experimentation for the same, [refer](https://github.com/onearmbandit/VAS-Core-Research/pull/47#issuecomment-993378362).
- Profiling of OSNet(Re-ID specific architecture) implementation for validating speed/accuracy trade-off, [refer](https://github.com/onearmbandit/VAS-Core-Research/pull/47#issuecomment-995441443).

As all the subsequent list of tasks, related to multi attribute vehicle recognition has been completed for the mentioned base papers, also the person multi attribute recognition experiments had been covered in separate [PR#41](https://github.com/onearmbandit/VAS-Core-Research/pull/41), but the same experimentation for the underlying architecture has been done for the person M.A.R problem in PR #47.

## Dataset Used
### Person Dataset
All the experimentation's conducted for this release has been done on [v0.1-alpha](https://github.com/onearmbandit/VAS-Data-Science/releases/tag/v0.1-alpha) person dataset release.
<table><td>
<img src="https://user-images.githubusercontent.com/57352045/120796875-f8638e00-c558-11eb-922f-4450a2651737.png"/></td>
<td>
<img src="https://user-images.githubusercontent.com/57352045/120797283-732ca900-c559-11eb-8469-bd9e77dbfd1e.png"/></td>
<td>
<img src="https://user-images.githubusercontent.com/57352045/120797438-aa9b5580-c559-11eb-8624-71e5871b2fb9.png"/></td>
</table>

**NOTE**: _**The number of data samples showed in above graphs are only from the training datasets. for detailed graphical analysis of complete dataset(train/test/val) has described in the [E.D.A document](https://github.com/onearmbandit/VAS-Data-Science/releases/download/v0.1-alpha/PETA.and.RAP.Dataset.EDA.Report.pdf) respectively**_.

### Vehicle Dataset
All the experimentation's conducted for this release has been done on [PR#68](https://github.com/onearmbandit/VAS-Data-Science/pull/68) vehicle dataset.
<table>
<tr>
<td>
<img src="https://user-images.githubusercontent.com/57352045/148689726-2e29b574-d1af-4a5d-a742-bf4da8e70cdc.png"/></td>
<td>
<img src="https://user-images.githubusercontent.com/57352045/148689729-b50d5ea7-e8e4-4d62-b077-c11141127afe.png"/></td>
</tr>
<tr>
<td align='center'>myauto.ge vehicle class imbalance</td>
<td align='center'>myauto.ge vehicle colour imbalance</td>
</tr>

<tr>
<td>
<img src="https://user-images.githubusercontent.com/57352045/148689734-8be5f78f-adfd-4e17-8e0f-593187636b98.png"/></td>
<td>
<img src="https://user-images.githubusercontent.com/57352045/148689735-f14bff01-572a-4c82-9298-09f1a0e9ed69.png"/></td>
</tr>
<tr>
<td align='center'>Standford vehicle class imbalance</td>
<td align='center'>Standford vehicle colour imbalance</td>
</tr>
</table>

**NOTE**: _**The number of data samples showed in above graphs are only from the training datasets. for detailed graphical analysis of complete dataset(train/test/val) has described in the [Myauto.ge vehicle dataset.pdf](https://github.com/onearmbandit/VAS-Core-Research/files/7835396/Myauto.ge.vehicle.dataset.pdf) & [stanford vehicle dataset.pdf](https://github.com/onearmbandit/VAS-Core-Research/files/7835398/stanford.vehicle.dataset.pdf) respectively**_.

## Experimentation Details
There are different experimentation's conducted on given architectures with all possible combinations for distinct loss functions, optimizer, learning rate, weight decays,etc. however amongst all the architectures IBN-Net seems to perform well in terms of speed/accuracy trade-off , hence the experimentation details of IBN-Net is only emphasized in this release, for detailed descriptions of all experimentation's conducted on other architectures refer to [this](https://docs.google.com/document/d/1rnSRnbJQLRgfR3DjcOuHTVHA51f_Q8tPZpK0H-t5ep0/edit#) doc.
**IBN-Net**, is a novel convolutional architecture, which remarkably enhances a CNN?s modeling ability on one domain (e.g. Cityscapes) as well as its generalization capacity on another domain (e.g. GTA5) without fine-tuning. IBN-Net carefully integrates Instance Normalization (IN) and Batch Normalization (BN) as building blocks, and can be wrapped into many advanced deep networks to improve their performances, as per the experimentation's conducted down the line IBN has been incorporated with ResNet-18 architecture, it can also be integrated with other S.O.T.A architectures like OS-Net, Efficient-Net, Res-Next, or even use-case specific custom CNN's.

### Network architecture
<p align='center'>
<img src="https://user-images.githubusercontent.com/57352045/148694823-025b41dc-4759-447a-8d4b-87c89ea3a164.png"/>
</p>

### Loss function
This architecture was tested on CELoss loss function implemented as per Strong-baseline research paper [here](https://github.com/ajithvallabai/Pedestrian_Attribute_Recognition).

#### CELoss sigmoid
CELoss introduced in  [Rethinking of Pedestrian Attribute Recognition Realistic Datasets and A Strong Baseline(code)](https://arxiv.org/pdf/2005.11909.pdf) They proposed weighted sigmoid cross entropy loss 
<p align='left'>
<img src="https://user-images.githubusercontent.com/57352045/148695020-598c9c68-5d00-4f88-bd4f-a814243be5f0.png"/>
</p>

Where W</sub>j</sub> is represented as,

![Screenshot from 2022-01-09 23-45-51](https://user-images.githubusercontent.com/57352045/148695025-e298e370-3c28-4d34-9344-aea2b4d1f0a7.png)

### Implementation
Model was trained for 100 epochs and best accuracy achieved for `CELoss` is 75  on the 98th  epoch. all Training logs experiments stored in DVC.Below results are taken with TTA(Test Time Augmentation) however it is observed that without TTA accuracy drops drastically,

<table>
<tr>
<td align='center'>Person M.A.R</td>
<td align='center'>Vehicle M.A.R</td>
</tr>
<tr>
<td>
<img src="https://user-images.githubusercontent.com/57352045/148695325-09ff1ffa-7576-4a85-8023-5f6fcc9f6a0b.png"/></td>
<td>
<img src="https://user-images.githubusercontent.com/57352045/148696107-a4cd149c-9d4e-4d6a-95e7-9f8ce3807e85.png"/></td>
</tr>
</table>

## Conclusive Remark
All the experimentation's conducted in PR #47 were only for the training & validation purpose, because the large part of the dataset for this problem is still in-progress, in order to get the desired accuracy numbers for this problem real-time dataset needs to be prepared to test it with existing as well as upcoming experimentation results respectively, all the results mentioned in the doc [here](https://docs.google.com/document/d/1rnSRnbJQLRgfR3DjcOuHTVHA51f_Q8tPZpK0H-t5ep0/edit?usp=sharing) & [here](https://docs.google.com/document/d/1Y489flM5FKmuf5Km6kfYas7cH-tN99suoicW0XZnSDc/edit?usp=sharing) are for testing and experimentation purpose only, the real-time accuracy numbers for this problem needs to be validated independently which will be going to addressed in the upcoming experimentation's and there respective underlying research based implementations. The infer latency has not been emphasized for this experimentation because the accuracy numbers are primary emphasize for the above conducted experiments, however once the expected accuracy numbers obtained will debug and look for infer latency optimization stuff, also computing the F.P.S and latency impact of this with end-to-end detection+tracking scenario is altogether a subject to different experimentation.

## Known Issues

- The code has not been refactored as per preliminary design pattern of S.O.L.I.D convention guidelines (#TODO), refer [discussion](https://github.com/onearmbandit/VAS-Core-Research/discussions/31).
- Without TTA accuracy drops drastically, this is very critical issue as in deployment release TTA is not required.
- The potential risk of model over-fitting because of the common identity issue in the dataset.
- Experimentation standardization needed for all  experimental graph plots & visualization including accuracy metric formulation (#TODO).
- Lack of technical implementation specific documentation (#TODO), refer [issue](https://github.com/onearmbandit/VAS-Core-Research/issues/51).

## Further improvements (#TODO):
- Detailed Profiling & model based Grad-CAM visualization need to be implemented for further experimentation's (#TODO).
- Improving accuracy without TTA, which is crucial for deployment specific releases.
- NN architecture optimization & decreasing trainable parameter to further improve latency numbers
- Use-case specific architecture need to be implemented to achieve best speed/accuracy trade-off.

## Evaluation Metrics

<table>
<tr>
<td>Version</td>
<td>Use-Case</td>
<td>Framework/IE</td>
<td>Architecture(GPU/CPU)</td>
<td>Algorithm</td>
<td>Accuracy</td>
<td>latency</td>
<td>FPS</td>
</tr>

<tr>
<td>v1.2-beta</td>
<td>Person M.A.R</td>
<td>PyTorch</td>
<td>Nvidia GTX 1060</td>
<td>IBN-Net-ResNet-18-a</td>
<td>75</td>
<td>2.73 ms</td>
<td>N/A</td>
</tr>
<tr>
<td>v1.2-beta</td>
<td>Vehicle M.A.R</td>
<td>PyTorch</td>
<td>Nvidia GTX 1060</td>
<td>IBN-Net-ResNet-18-a</td>
<td>76.61</td>
<td>3.32 ms</td>
<td>N/A</td>
</tr>

</table>

** All AP(f1 score) numbers are for single-model single-scale, single-channel without ensemble or test-time augmentation. Accuracy here we referring to is based on standard f1-score evaluation metrics with TTA, which is based on confusion matrix, precision and recall values. for accuracy validations we are considering confusion matrix parameters with IOU threshold >= 0.5 for detection models only, for classification model IOU is not needed.

** Latency<sub>GPU/CPU</sub> measures end-to-end latency per image/frame averaged over test, validation input sources which can be from RTSP stream or video sources. using specified CPU/GPU architectures, and includes image preprocessing, inference at batch size 1, post-processing and NMS. 

**NOTE**: _The accuracy numbers obtained from above experiment may looks prominent but important thing to note is, the dataset on which the model has trained on has lot of similar appearing image samples which is been described as common identity problem in the release notes of the dataset, so there is high chance that the model got over-fit and potentially may not perform well in real-time scenarios, another reason for model overfiting is stagnant loss which has been overcome by implementing auto learning rate scheduler in the subsequent release_.